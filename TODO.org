* Issues
  + *[stemmers]* Should I rewrite stemmers in the spirit of create and apply?
    It should yield performance improvement but what about concurrency?
    Alternative is to keep all the stemmers in delays.
  + *[detectors]* Lookup new method from Cybozu for tweeets
    http://shuyo.wordpress.com/2012/05/17/short-text-language-detection-with-infinity-gram/
  + *[detectors]* Add tests for language detection.
  + *[detectors]* Add tests for encoding.
  + *[detectors]* Add option of seed initialization fro Cybozu lang detector
  + *[general]* Check for ICU automatic detection + decoding facilities
    This may solve the probles of different encodings in files.
  + *[detectors]* Investigate the issue of ICU language detection problems
    Why it works only for texts with removed diacritics?
    Does manual removal affect the non-latin alphabet text detection?
  + *[characters]* Add function to detect/normalize some UTF-8 characters
    Examples
    - "…" -> "..."
    - "„" -> "\""
    - "”" -> "\"" 
    - "—" -> "-" 
    - hard spaces to spaces
    - what more ?

  + *[tagging]* Refactor tagging module so it works with langlab
  + *[stopwords]* Verify norwegian stopwords (no-sw) 
    I had to do some uppercase conversions, so something migh be wrong there.

* Done
  + *[stopwords]* Add docs to stopwords functions 
  + *[stopwords]* Convert en-drop-articles so it uses stopwords
  + *[stopwords]* Add basic unit tests to stopwords functions
  + *[detectors]* Create wrappers for language detection module in Apache Tika
  + *[stopwords]* Add constants containing articles
  + *[stopwords]* Create a module and add general stopwords filter 
  + *[general]* Refactor langlab-base -> langlab
  + *[readability]* Correct test to the infix notation
  + *[readablilty]* Correct counting characters to bi version
  + *[characters]* Add functions detecting/removing non-MBP characters
  + *[transformers]* Add tests to transformers  
  + *[parsers]* Add tests to sentence splitters
  + *[parsers]* Add simple tokenizer based on Analyzer from Lucene
    http://stackoverflow.com/questions/6334692/how-to-use-a-lucene-analyzer-to-tokenize-a-string
    See post by Ben McCann for Lucene 4.1
  + *[characters]* Make use of punctuation classes from Unicode 
    [Pc] Punctuation, Connector
    [Pd] Punctuation, Dash
    [Pe] Punctuation, Close
    [Pf] Punctuation, Final quote (may behave like Ps or Pe depending on usage)
    [Pi] Punctuation, Initial quote (may behave like Ps or Pe depending on usage)
    [Po] Punctuation, Other
    [Ps] Punctuation, Open
     see http://www.fileformat.info/info/unicode/category/index.htm
  + *[characters]* Add tokenizer based on module ICU4j and its break iterator
    It implements unicode segmentatior rules 
    http://www.unicode.org/reports/tr29/
    http://icu-project.org/apiref/icu4j/com/ibm/icu/text/BreakIterator.html
    More http://site.icu-project.org/
  + *[characters]* Add Java functions to characters module
    containsPunctuation(String s)
    containsPunctuationOnly(String s)
    containsWhitespace(String s)
    containsWhitespaceOnly(String s)
